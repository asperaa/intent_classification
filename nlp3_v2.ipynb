{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classification On Enron E-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data\n",
    "\n",
    "Looks like our main objective from the dataset is to predict whether an email text have Positive or Negative intent based ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial approach\n",
    "\n",
    "Firstly,I will be using a simple Convolutional Neural Network to for text classification task.\n",
    "\n",
    "https://arxiv.org/abs/1408.5882 (Convolutional Neural Networks for Sentence Classification by Yoon Kim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data\n",
    "Data is being extracted from a .txt file and then converted into .csv and put into the respective dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3632, 2)\n",
      "No Act now to keep your life on the go!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "txt_file = r\"train.txt\"\n",
    "csv_file = r\"train.csv\"\n",
    "\n",
    "# use 'with' if the program isn't going to immediately terminate\n",
    "# so you don't leave files open\n",
    "# the 'b' is necessary on Windows\n",
    "# it prevents \\x1a, Ctrl-z, from ending the stream prematurely\n",
    "# and also stops Python converting to / from different line terminators\n",
    "# On other platforms, it has no effect\n",
    "in_txt = csv.reader(open(txt_file, \"rt\",encoding='utf-8'), delimiter = '\\t')\n",
    "\n",
    "out_csv = csv.writer(open(csv_file, 'wt',encoding='utf-8'))\n",
    "out_csv.writerow(('intent', 'text'))\n",
    "out_csv.writerows(in_txt)\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"train.csv\") \n",
    "data.head()\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "intent=data['intent'] #extract intent\n",
    "text=data['text'] #extract text\n",
    "\n",
    "print(intent[1],text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the labels to numeric values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3632\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "3602    1\n",
      "3603    1\n",
      "3604    1\n",
      "3605    1\n",
      "3606    1\n",
      "3607    1\n",
      "3608    1\n",
      "3609    1\n",
      "3610    1\n",
      "3611    1\n",
      "3612    1\n",
      "3613    1\n",
      "3614    1\n",
      "3615    1\n",
      "3616    1\n",
      "3617    1\n",
      "3618    1\n",
      "3619    1\n",
      "3620    1\n",
      "3621    1\n",
      "3622    1\n",
      "3623    1\n",
      "3624    1\n",
      "3625    1\n",
      "3626    1\n",
      "3627    1\n",
      "3628    1\n",
      "3629    1\n",
      "3630    1\n",
      "3631    1\n",
      "Name: intent, Length: 3632, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_categories=data.intent.unique()\n",
    "unique_categories_dictionary=dict(zip(unique_categories,range(0,len(unique_categories))))\n",
    "labels=data['intent'].map(unique_categories_dictionary, na_action='ignore')\n",
    "print(len(labels))\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "We tokenize the text before we can feed it into a neural network. This tokenization process will also remove some of the features of the original text, such as all punctuation or words that are less common.\n",
    "\n",
    "We have to specify the size of our vocabulary. Words that are less frequent will get removed. In this case we want to retain the 5,000 most common words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3632\n",
      "9\n",
      "[116, 126, 18, 64, 1, 46, 761, 99, 47]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text) # Generate sequences\n",
    "\n",
    "print (len(sequences))  ## Testing the output of the tokenizer functions.\n",
    "print (len(sequences[0]))\n",
    "print (sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word_index\n",
    "Our text is now converted to sequences of numbers. It makes sense to convert some of those sequences back into text to check what the tokenization did to our text. To this end we create an inverse index that maps numbers to words while the tokenizer maps words to numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6,800 unique words.\n",
      "1 contact me now to make 100 today link "
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))\n",
    "\n",
    "#Create inverse index mapping numbers to words\n",
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring the text length\n",
    "\n",
    "Let's ensure all sequences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.282764317180618, 10.145697980559813)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([[1,2,3]], maxlen=5) # testing the pad_sequences function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 26\n",
    "text_seq = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning labels into one-hot encodings\n",
    "Labels can quickly be encoded into one-hot vectors with Keras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (3632, 26)\n",
      "Shape of labels: (3632, 2)\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "labels=data['intent'].map(unique_categories_dictionary, na_action='ignore')\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "print('Shape of data:', text_seq.shape)\n",
    "print('Shape of labels:', labels.shape)\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GloVe embeddings\n",
    "\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "glove_dir = 'C:/Users/admin/Desktop' # This is the folder with the dataset/glove embeddings\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.043084    0.53232998  0.54254001 -0.076952   -0.29673001  0.52986002\n",
      "  0.21379     0.15789001 -0.39520001 -0.91889    -0.65850002  0.68706\n",
      "  0.10821    -0.10694    -0.34009999  1.04400003  0.12774999  0.51156998\n",
      "  0.60314     0.71366    -0.53740001  0.37737     0.12186     0.60891002\n",
      "  0.50107002  2.02150011 -0.47318     0.46952999  0.12542     0.60206997\n",
      "  0.11007     0.37586999  1.01370001 -0.24779999  0.65748     0.12801\n",
      " -0.57647002 -0.25753999  0.62426001  0.010864   -0.40680999  0.16173001\n",
      " -0.84694999 -0.24603     0.29078001  0.85460001 -0.067021    0.69331002\n",
      " -0.71544999 -0.25184    -0.74741    -0.26506999  0.48730001  0.41991001\n",
      " -0.86741    -0.52350003 -0.44773999 -0.044584    0.033836    0.29909\n",
      "  0.73754001  0.81651002  0.69431001  0.80453002  0.29276001 -0.025244\n",
      " -0.30452999 -0.34329     0.11933    -0.29655001  0.1072     -0.18945999\n",
      "  0.18501    -0.75480002 -0.25628     0.34437999 -0.016743    0.0040503\n",
      "  0.39342001  0.99404001 -0.32159001 -0.49434     0.41707999 -0.011019\n",
      " -0.16613001 -0.20839     0.28152001 -0.82995999  0.79838997  0.61645001\n",
      "  0.31536999 -0.27629    -0.54592001  0.23026     0.023473   -0.15933999\n",
      " -1.43889999 -0.75358999  0.51490003 -0.52552003]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_index['frog'])\n",
    "print (len(embeddings_index['frog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GloVe Embeddings\n",
    "\n",
    "Following snippet shows how to use pre-trained word embeddings in the model. There are four essential steps:\n",
    "\n",
    "1.Loading the pretrained word embeddings.<br/>\n",
    "2.Creating a tokenizer object.(already done in the above code)<br/>\n",
    "3.Transforming text documents to sequence of tokens and pad them.(already done in the above code)<br/>\n",
    "4.Create a mapping of token and their respective embeddings.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words+1, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the model :\n",
    "\n",
    "The architecture is comprised of three key pieces:<br/>\n",
    "\n",
    "1.Word Embedding: A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation.<br/>\n",
    "2.Convolutional Model: A feature extraction model that learns to extract salient features from documents represented using a word embedding.<br/>\n",
    "3.Fully Connected Model: The interpretation of extracted features in terms of a predictive output.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 26, 100)           500100    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24, 50)            15050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 50)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 535,302\n",
      "Trainable params: 35,202\n",
      "Non-trainable params: 500,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words+1, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False))\n",
    "model.add(Conv1D(50, 3, activation='relu'))  #https://arxiv.org/abs/1408.5882 (Convolutional Neural Networks for Sentence Classification by Yoon Kim)\n",
    "model.add(MaxPooling1D(3))                    #https://arxiv.org/abs/1510.03820(A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the test data\n",
    "\n",
    "We will prepare the test data in same way we prepared train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992, 2)\n",
      "992\n",
      "992\n",
      "12\n",
      "[3, 143, 313, 2, 25, 7, 4, 818, 47, 30, 627, 133]\n",
      "Found 3,029 unique words.\n",
      "i look forward to meeting you and learning about your successful business \n",
      "\n",
      "Shape of test data: (992, 26)\n",
      "Shape of test labels: (992, 2)\n"
     ]
    }
   ],
   "source": [
    "txt_test_file = r\"test.txt\"\n",
    "csv_test_file = r\"test.csv\"\n",
    "\n",
    "# use 'with' if the program isn't going to immediately terminate\n",
    "# so you don't leave files open\n",
    "# the 'b' is necessary on Windows\n",
    "# it prevents \\x1a, Ctrl-z, from ending the stream prematurely\n",
    "# and also stops Python converting to / from different line terminators\n",
    "# On other platforms, it has no effect\n",
    "in_txt = csv.reader(open(txt_test_file, \"rt\",encoding='utf-8'), delimiter = '\\t')\n",
    "\n",
    "out_csv = csv.writer(open(csv_test_file, 'wt',encoding='utf-8'))\n",
    "out_csv.writerow(('intent', 'text'))\n",
    "out_csv.writerows(in_txt)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\") \n",
    "test_data.head()\n",
    "\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "test_intent=test_data['intent'] #extract intent\n",
    "test_text=test_data['text'] #extract text\n",
    "\n",
    "\n",
    "unique_categories=test_data.intent.unique()\n",
    "unique_categories_dictionary=dict(zip(unique_categories,range(0,len(unique_categories))))\n",
    "test_labels=test_data['intent'].map(unique_categories_dictionary, na_action='ignore')\n",
    "print(len(test_labels))\n",
    "\n",
    "\n",
    "vocab_size = 5000\n",
    "test_tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "test_tokenizer.fit_on_texts(test_text)\n",
    "test_sequences = test_tokenizer.texts_to_sequences(test_text) # Generate sequences\n",
    "\n",
    "\n",
    "print (len(test_sequences))  ## Testing the output of the tokenizer functions.\n",
    "print (len(test_sequences[0]))\n",
    "print (test_sequences[0])\n",
    "\n",
    "\n",
    "test_word_index = test_tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(test_word_index)))\n",
    "\n",
    "\n",
    "#Create inverse index mapping numbers to words\n",
    "test_inv_index = {v: k for k, v in test_tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in test_sequences[0]:\n",
    "    x = test_inv_index.get(w)\n",
    "    print(x,end = ' ')\n",
    "    \n",
    "print(\"\\n\")   \n",
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std\n",
    "\n",
    "\n",
    "max_length = 26\n",
    "test_text_seq = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "test_labels = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of test data:', test_text_seq.shape)\n",
    "print('Shape of test labels:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3632 samples, validate on 992 samples\n",
      "Epoch 1/15\n",
      "3632/3632 [==============================] - 1s 348us/step - loss: 0.0040 - acc: 0.9997 - val_loss: 3.4858 - val_acc: 0.5015\n",
      "Epoch 2/15\n",
      "3632/3632 [==============================] - 1s 218us/step - loss: 0.0062 - acc: 0.9986 - val_loss: 2.9076 - val_acc: 0.6008\n",
      "Epoch 3/15\n",
      "3632/3632 [==============================] - 1s 215us/step - loss: 0.0489 - acc: 0.9821 - val_loss: 3.1300 - val_acc: 0.5076\n",
      "Epoch 4/15\n",
      "3632/3632 [==============================] - 1s 221us/step - loss: 0.0141 - acc: 0.9972 - val_loss: 3.2770 - val_acc: 0.5141\n",
      "Epoch 5/15\n",
      "3632/3632 [==============================] - 1s 224us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 3.3027 - val_acc: 0.5227\n",
      "Epoch 6/15\n",
      "3632/3632 [==============================] - 1s 227us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.3307 - val_acc: 0.5277\n",
      "Epoch 7/15\n",
      "3632/3632 [==============================] - 1s 250us/step - loss: 9.7118e-04 - acc: 1.0000 - val_loss: 3.3963 - val_acc: 0.5272\n",
      "Epoch 8/15\n",
      "3632/3632 [==============================] - 1s 218us/step - loss: 7.9812e-04 - acc: 1.0000 - val_loss: 3.4126 - val_acc: 0.5287\n",
      "Epoch 9/15\n",
      "3632/3632 [==============================] - 1s 228us/step - loss: 6.7808e-04 - acc: 1.0000 - val_loss: 3.4909 - val_acc: 0.5287\n",
      "Epoch 10/15\n",
      "3632/3632 [==============================] - 1s 230us/step - loss: 5.8627e-04 - acc: 1.0000 - val_loss: 3.5304 - val_acc: 0.5277\n",
      "Epoch 11/15\n",
      "3632/3632 [==============================] - 1s 222us/step - loss: 5.1226e-04 - acc: 1.0000 - val_loss: 3.5766 - val_acc: 0.5277\n",
      "Epoch 12/15\n",
      "3632/3632 [==============================] - 1s 238us/step - loss: 4.3313e-04 - acc: 1.0000 - val_loss: 3.6134 - val_acc: 0.5292\n",
      "Epoch 13/15\n",
      "3632/3632 [==============================] - 1s 241us/step - loss: 3.7196e-04 - acc: 1.0000 - val_loss: 3.6415 - val_acc: 0.5312\n",
      "Epoch 14/15\n",
      "3632/3632 [==============================] - 1s 216us/step - loss: 3.1479e-04 - acc: 1.0000 - val_loss: 3.7066 - val_acc: 0.5318\n",
      "Epoch 15/15\n",
      "3632/3632 [==============================] - 1s 216us/step - loss: 2.6271e-04 - acc: 1.0000 - val_loss: 3.7798 - val_acc: 0.5292\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_history=model.fit(text_seq, labels, validation_split=0.0,validation_data=(test_text_seq,test_labels),shuffle=True, epochs=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting a test accuracy of around 53% which is not a satisfactory result.Low accuracy using the above deep learning model can be explained by the following graph:\n",
    "\n",
    "![title](ds.png)\n",
    "\n",
    "As we construct larger neural networks and train them with more and more data, their performance continues to increase. This is generally different to other machine learning techniques that reach a plateau in performance.<br/>\n",
    "So,the amount of training examples are really low for a deep learning model to give a good accuracy.Now,we will try non-deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries\n",
    "\n",
    "sklearn library for various ML algorithms like Random Forest,SGD(Stocastic Gradient Descent),Logistic Regression and various other algorithms.<br/>\n",
    "\n",
    "numpy library for handling linear alzebra related tasks.<br/>\n",
    "\n",
    "pandas library for data manipulation and analysis. (In this case,reading the csv file into data-frames).<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the training data format\n",
    "\n",
    "The given training data is in .txt format with tab separation between the label(i.e. Intent) and corresponding \"text\".So,the data is converted into .csv format for easy manipulation and further conversion into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Intent                                               text\n",
      "0     No     >>> [1]Contact Me Now to Make $100 Today!$LINK\n",
      "1     No               Act now to keep your life on the go!\n",
      "2     No  Choose between $500 and $10000 dollars with up...\n",
      "3     No                         Click above to earn today.\n",
      "4     No        Click here to receive your first $10 today:\n",
      "\n",
      "Shape of train data (3657, 2)\n"
     ]
    }
   ],
   "source": [
    "txt_file = r\"train.txt\"  # for converting the txt file to a csv file for better handling of data\n",
    "csv_file = r\"train.csv\"\n",
    "\n",
    "in_txt = csv.reader(open(txt_file, \"rt\",encoding='utf-8'), delimiter = '\\t') # Opening the text file in text mode\n",
    "\n",
    "out_csv = csv.writer(open(csv_file, 'wt',encoding='utf-8')) # writing the contents of text file to a csv file.\n",
    "out_csv.writerow(('Intent', 'text'))\n",
    "out_csv.writerows(in_txt)\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\") \n",
    "print(train_data.head())\n",
    "print()\n",
    "\n",
    "print(\"Shape of train data\",train_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the test data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data: (907, 2)\n"
     ]
    }
   ],
   "source": [
    "# Perform the same actities to convert the test_data txt file to test_data csv file.\n",
    "\n",
    "txt_test_file = r\"test.txt\"\n",
    "csv_test_file = r\"test.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in_txt = csv.reader(open(txt_test_file, \"rt\",encoding='utf-8'), delimiter = '\\t')\n",
    "\n",
    "out_csv = csv.writer(open(csv_test_file, 'wt',encoding='utf-8'))\n",
    "out_csv.writerow(('Intent', 'text'))\n",
    "out_csv.writerows(in_txt)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\") \n",
    "test_data.head()\n",
    "\n",
    "print(\"Shape of test data:\",test_data.shape)\n",
    "\n",
    "\n",
    "test_intent=test_data['Intent'] #extract intent\n",
    "test_text=test_data['text'] #extract text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Intent and Text into Series\n",
    "\n",
    "Taking out the columns of the dataframe into corresponding series for better handling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3657, 2)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "Intent=train_data['Intent'] #extract intent\n",
    "text=train_data['text'] #extract text\n",
    "print(train_data.shape)\n",
    "\n",
    "print(type(Intent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Binarizer\n",
    "\n",
    "Converts target variable to binary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb= LabelBinarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming takes morphologically complex words and reduces them to their root morphemes. For example, if the input contains the words \"tasty\", \"tastier\", and \"tastiest\", the suffixes would be stripped off, and all of these forms would be treated as a single stem, \"tasti\".<br/>\n",
    "\n",
    "In this case,I have used PorterStemmer because it is the least aggressive of all the other commonly used stemmers(Snowball,Lancaster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop words\n",
    "\n",
    "Stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools.Text may contain stop words like ‘the’, ‘is’, ‘are’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular expressions help us deal with characters and numerics and modify them according to our requirement.\n",
    "\n",
    "import re\n",
    "\n",
    "#import the nltk library.The nltk module contains a list of stop words.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english')) #We get a set of English stop words using this line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing preprocessing\n",
    "\n",
    "In the preprocessing step, we are removing the punctuation,lower-casing the words,removing the stop words for the text column.<br/>\n",
    "In the label column,we are using label_binarizer to convert the \"Yes\" and \"No\" to \"1\" and \"0\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&gt;&gt;&gt; [1]Contact Me Now to Make $100 Today!$LINK</td>\n",
       "      <td>contact make todaylink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Act now to keep your life on the go!</td>\n",
       "      <td>act keep life go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Choose between $500 and $10000 dollars with up...</td>\n",
       "      <td>choos dollar year repay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Click above to earn today.</td>\n",
       "      <td>click earn today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Click here to receive your first $10 today:</td>\n",
       "      <td>click receiv first today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intent                                               text  \\\n",
       "0       0     >>> [1]Contact Me Now to Make $100 Today!$LINK   \n",
       "1       0               Act now to keep your life on the go!   \n",
       "2       0  Choose between $500 and $10000 dollars with up...   \n",
       "3       0                         Click above to earn today.   \n",
       "4       0        Click here to receive your first $10 today:   \n",
       "\n",
       "              pre_processed  \n",
       "0    contact make todaylink  \n",
       "1          act keep life go  \n",
       "2   choos dollar year repay  \n",
       "3          click earn today  \n",
       "4  click receiv first today  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processing(df):\n",
    "    \n",
    "    #lowering and removing punctuation\n",
    "    df['pre_processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    #removing the numerical values and working only with text values\n",
    "    df['pre_processed'] = df['pre_processed'].apply(lambda x : re.sub('[^a-zA-Z]', \" \", x ))\n",
    "    #removing the stopwords\n",
    "    df['pre_processed'] = df['pre_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in sw ]))\n",
    "    #stemming the words\n",
    "    df['pre_processed'] = df['pre_processed'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "   \n",
    "    df[\"Intent\"]= lb.fit_transform(df[\"Intent\"])\n",
    "    return df\n",
    "    \n",
    "df= processing(train_data)   \n",
    "#df = df.drop(\"processed\", axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the features and target dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FTsplit(df):       #This subroutine splits the given dataframe into features and target dataframe\n",
    "    features='pre_processed'\n",
    "    target = 'Intent'\n",
    "    X= df[features]\n",
    "    y= df[target]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =FTsplit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  contact make todaylink\n",
      "1                                        act keep life go\n",
      "2                                 choos dollar year repay\n",
      "3                                        click earn today\n",
      "4                                click receiv first today\n",
      "5                                   click start shop link\n",
      "6               click watch im sure long hell leav public\n",
      "7                       confirm view first great opportun\n",
      "8                       copi past link browser see result\n",
      "9                    find add clip pressroom second video\n",
      "10                                       go direct access\n",
      "11                                              go inform\n",
      "12      adob acrobat reader pleas click follow instruc...\n",
      "13                       want confirm simpli ignor messag\n",
      "14                  dont want chang password ignor messag\n",
      "15      see noth banner pleas click httpwwwbzprodtvmai...\n",
      "16      see noth banner pleas click httpwwwbzprodtvmai...\n",
      "17                           want see got debt click link\n",
      "18            honor memori day take advantag special deal\n",
      "19           honor mother day take advantag huge discount\n",
      "20      event phone would like make sure mr lay get in...\n",
      "21                                                   join\n",
      "22                          jude search claim today entri\n",
      "23                                      keep move forward\n",
      "24         open glyder get feedback templat free one week\n",
      "25         mail work america somervil avenu suit somervil\n",
      "26      tell us much plan help make god word daili par...\n",
      "27      pleas add eventscommunicationsonekingslanecom ...\n",
      "28      pleas add mailtojobssnagajobemailcom jobssnaga...\n",
      "29                  pleas add white list safe sender list\n",
      "                              ...                        \n",
      "3627    would make sure none goofi financi stuff impac...\n",
      "3628    would mind forward folk corpor legal might int...\n",
      "3629                          would rather lunch tomorrow\n",
      "3630                                   would send purchas\n",
      "3631               wu wonder give direct bwi accomid base\n",
      "3632                               ye pleas forward along\n",
      "3633                                       ye well sunday\n",
      "3634                                     avail quick call\n",
      "3635                           chanc know review tomorrow\n",
      "3636    check order statu click httpwwwappusdellcomuse...\n",
      "3637                           come whenev want get check\n",
      "3638    dont need right away get chanc go unifi feb sh...\n",
      "3639                         go brief tonit tomorrow meet\n",
      "3640    guy could work offer compromis posit involv we...\n",
      "3641    request provid feedback particip attend next p...\n",
      "3642                                        get one thing\n",
      "3643    need download instal wyse pocketcloud companio...\n",
      "3644    may need check mari trisha transport capac tic...\n",
      "3645              may want give cowork nudg kick occasion\n",
      "3646    might give head there anyth one upcom wednesda...\n",
      "3647    might tell mike provid two desk space well res...\n",
      "3648             need get visa issu work quickli kay help\n",
      "3649                          need look calendar see come\n",
      "3650                            need take minut look asap\n",
      "3651    still need coordin contract group get correct ...\n",
      "3652    googl data archiv readi pleas visit link download\n",
      "3653    account think receiv messag error pleas ignor ...\n",
      "3654    email pleas visit link durke road ne cleveland tn\n",
      "3655    yet got chanc review us appstor request help u...\n",
      "3656    kmlemanski pleas share link anyon midnight ton...\n",
      "Name: pre_processed, Length: 3657, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X)  #Checking the features dataframe by printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test data\n",
    "\n",
    "Prepare the test data by using tf-idf vectorizer.It takes a list of documents and returns a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of document-term matrix (907, 2130)\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data using tf-idf vectorizer.\n",
    "\n",
    "test_vectorisor=TfidfVectorizer(\"english\")\n",
    "\n",
    "df_test=processing(test_data)\n",
    "\n",
    "X_test, y_test =FTsplit(df_test)\n",
    "test_text_feat = X_test\n",
    "test_features = test_vectorisor.fit_transform(test_text_feat)\n",
    "\n",
    "\n",
    "print(\"Dimension of document-term matrix\",test_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of document-term matrix: (3657, 2130)\n"
     ]
    }
   ],
   "source": [
    "#Prepare train data using tf-idf vectorizer.\n",
    "\n",
    "train_vectorisor=TfidfVectorizer(\"english\",max_features=test_features.shape[1])\n",
    "df_train=processing(train_data)\n",
    "\n",
    "X_train, y_train =FTsplit(df_train)\n",
    "train_text_feat = X_train\n",
    "train_features = train_vectorisor.fit_transform(train_text_feat)\n",
    "\n",
    "print(\"Dimension of document-term matrix:\",train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing different classifers from sklearn library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilaising different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=2000,max_depth=70,min_samples_split=5,min_samples_leaf=100, random_state=111)\n",
    "abc = AdaBoostClassifier(n_estimators=62, random_state=112)\n",
    "sgd = SGDClassifier(loss='log', penalty='l2',alpha=1e-3, n_iter=5, random_state=111)\n",
    "# xgbb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "#                         subsample=0.8, nthread=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'SVC' : svc,'LR': lrc, 'RF': rfc,'SGD': sgd}\n",
    "\n",
    "def train_classifier(clf, feature_train, labels_train):    \n",
    "    clf.fit(feature_train, labels_train)\n",
    "    \n",
    "def predict_labels(clf, features):\n",
    "    return (clf.predict(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_scores = []\n",
    "# for k,v in clfs.items():\n",
    "#     train_classifier(v, features_train, labels_train)\n",
    "#     pred = predict_labels(v,features_test)\n",
    "#     pred_scores.append((k, [accuracy_score(labels_test,pred)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Prediction\n",
    "\n",
    "Training and prediction is being done using different classfiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "pred_scores = []\n",
    "for k,v in clfs.items():\n",
    "    train_classifier(v, train_features, y)\n",
    "    pred = predict_labels(v,test_features)\n",
    "    pred_scores.append((k, [accuracy_score(y_test,pred)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.714443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.738699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.753032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.746417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Score\n",
       "SVC  0.714443\n",
       "LR   0.738699\n",
       "RF   0.753032\n",
       "SGD  0.746417"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score']) #Store predictions of various algos in a df.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the above table that the random forest classifier gives the highest accuracy of 75.3%.Stocastic Gradient Descent gives the accuracy of 74.6%,which is a close second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability over a single estimator.<br/>\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:<br/>\n",
    "\n",
    "In <b>averaging methods</b>, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.<br/>\n",
    "Examples: Bagging methods, Forests of randomized trees, …<br/>\n",
    "\n",
    "By contrast, in <b>boosting methods</b>, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.<br/>\n",
    "\n",
    "Below we are using a average method with Voting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('SGD', sgd), ('LR', lrc), ('RF', rfc), ('Ada', abc)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740904079383\n"
     ]
    }
   ],
   "source": [
    "eclf.fit(train_features,y)\n",
    "\n",
    "pred = eclf.predict(test_features)\n",
    "print(accuracy_score(y_test,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly,we can see that the accuracy of the Voting Classfier (74.09 %) is less than the accuracy of random forest tree(75.3%).<br/>\n",
    "\n",
    "Now,we will try to improve the accuracy by hyperparameter tuning of random forest claasifier using GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 70, 'max_leaf_nodes': 20, 'min_samples_leaf': 50, 'min_samples_split': 5, 'n_estimators': 1300}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc = RandomForestClassifier(n_estimators=2000,max_depth=70,min_samples_split=5,min_samples_leaf=100, random_state=111)\n",
    "\n",
    "# parameters for GridSearchCV\n",
    "param_grid2 = {\"n_estimators\": [1300, 1400, 1500],\n",
    "              \"max_depth\": [70,80],\n",
    "              \"min_samples_split\": [5,10,15],\n",
    "              \"min_samples_leaf\": [50,100,200],\n",
    "              \"max_leaf_nodes\": [20, 40],\n",
    "              }\n",
    "grid_search = GridSearchCV(rfc, param_grid=param_grid2)\n",
    "grid_search.fit(train_features,y)\n",
    "\n",
    "# print(grid_search.cv_results_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the grid search to find the best hyperparameters for the random forest classifier.Now,we use those hyperparameters for new prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753031973539\n"
     ]
    }
   ],
   "source": [
    "eclf.fit(train_features,y)\n",
    "rfc = RandomForestClassifier(n_estimators=1300,max_depth=70,min_samples_split=5,min_samples_leaf=50, random_state=111)\n",
    "\n",
    "rfc.fit(train_features,y)\n",
    "\n",
    "pred = rfc.predict(test_features)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On changing our candidates(n_estimators,max_depth,max_leaf nodes) we can hope to improve our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
